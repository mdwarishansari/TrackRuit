import re
from typing import Dict, List, Any
from .base_model import BaseModel
from pipelines.preprocess import preprocessor

class FeedbackModel(BaseModel):
    """ML-Powered Resume Feedback and Analysis Model"""
    
    def __init__(self, version: str = None):
        super().__init__("feedback", version)
        self._init_ml_components()
    
    def _init_ml_components(self):
        """Initialize ML-specific components for resume analysis"""
        # ML Feature: Role-specific keyword libraries
        self.role_keywords = {
            'python': ['python', 'django', 'flask', 'fastapi', 'pandas', 'numpy', 'tensorflow', 'pytorch'],
            'java': ['java', 'spring', 'hibernate', 'j2ee', 'maven', 'microservices'],
            'javascript': ['javascript', 'react', 'vue', 'angular', 'node.js', 'typescript', 'express'],
            'data scientist': ['python', 'machine learning', 'statistics', 'pandas', 'numpy', 'sql', 'tensorflow'],
            'devops': ['docker', 'kubernetes', 'aws', 'jenkins', 'terraform', 'linux', 'ci/cd']
        }
        
        # ML Feature: Scoring weights (based on resume effectiveness research)
        self.weights = {
            'structure': 0.4,
            'keywords': 0.4,
            'skills': 0.2
        }
        
        # ML Feature: Impact indicators for achievement detection
        self.impact_verbs = [
            'increased', 'decreased', 'improved', 'reduced', 'saved',
            'achieved', 'developed', 'managed', 'led', 'implemented'
        ]
    
    def _create_default_model(self):
        """Create default ML feedback model"""
        self.is_loaded = True
        print(f"âœ… Feedback Model v{self.get_version()} loaded with ML analysis")
    
    def preprocess(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ML Preprocessing: Prepare resume for analysis"""
        resume_text = data.get('resume_text', '')
        target_role = data.get('target_role', 'software developer')
        
        return {
            'resume_text': preprocessor.clean_text(resume_text),
            'target_role': target_role.lower(),
            'original_text': resume_text,
            'original_data': data
        }
    
    def predict(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ML Prediction: Generate comprehensive resume feedback"""
        try:
            processed_data = self.preprocess(data)
            resume_text = processed_data['resume_text']
            target_role = processed_data['target_role']
            
            if not resume_text:
                return self._get_empty_response()
            
            # ML Analysis: Extract features
            skills = preprocessor.extract_skills(resume_text)
            sections = preprocessor.extract_sections(resume_text)
            
            # ML Scoring: Calculate component scores
            structure_score = self._calculate_ml_structure_score(sections, resume_text)
            keyword_score = self._calculate_ml_keyword_score(resume_text, target_role)
            skill_score = self._calculate_ml_skill_score(skills, target_role)
            
            # ML Aggregation: Overall score
            overall_score = (
                structure_score * self.weights['structure'] +
                keyword_score * self.weights['keywords'] + 
                skill_score * self.weights['skills']
            )
            
            # ML Feedback Generation
            feedback = self._generate_ml_feedback(
                structure_score, keyword_score, skill_score, 
                sections, skills, target_role
            )
            
            # Return with proper ML model version
            return {
                "overall_score": round(overall_score, 4),
                "structure_score": round(structure_score, 4),
                "keyword_score": round(keyword_score, 4),
                "skill_score": round(skill_score, 4),
                "metrics": {
                    "word_count": len(resume_text.split()),
                    "character_count": len(resume_text),
                    "skills_found": len(skills),
                    "sections_found": len(sections),
                    "impact_verbs": self._count_impact_verbs(resume_text)
                },
                "skills_found": skills[:15],
                "sections_found": list(sections.keys()),
                "feedback": feedback,
                "model_version": self.get_version()  # This will now be a string
            }
            
        except Exception as e:
            print(f"ML Error in feedback generation: {e}")
            return self._get_error_response(str(e))
    
    def _calculate_ml_structure_score(self, sections: Dict[str, str], resume_text: str) -> float:
        """ML Feature: Calculate structure score using ML principles"""
        score = 0.0
        
        # ML Component: Essential section detection
        essential_sections = ['experience', 'education', 'skills']
        found_essential = sum(1 for section in essential_sections if section in sections)
        section_completeness = found_essential / len(essential_sections)
        score += section_completeness * 0.4
        
        # ML Component: Optimal length scoring
        word_count = len(resume_text.split())
        if 300 <= word_count <= 1000:  # Industry standard optimal range
            score += 0.3
        elif 200 <= word_count <= 1500:  # Acceptable range
            score += 0.2
        else:
            score += 0.1
        
        # ML Component: Section content quality
        has_substantial_content = any(
            len(content.split()) > 50 for content in sections.values()
        )
        if has_substantial_content:
            score += 0.3
        
        return min(score, 1.0)
    
    def _calculate_ml_keyword_score(self, resume_text: str, target_role: str) -> float:
        """ML Feature: Calculate keyword relevance score"""
        # Get relevant keywords for target role
        relevant_keywords = []
        for role, keywords in self.role_keywords.items():
            if role in target_role.lower():
                relevant_keywords.extend(keywords)
        
        # Default to software engineering keywords
        if not relevant_keywords:
            relevant_keywords = self.role_keywords['python']
        
        # ML Component: Keyword coverage calculation
        resume_lower = resume_text.lower()
        found_keywords = [kw for kw in relevant_keywords if kw in resume_lower]
        
        if not relevant_keywords:
            return 0.7  # Default score if no specific keywords
        
        coverage = len(found_keywords) / len(relevant_keywords)
        
        # ML Component: Keyword density bonus
        word_count = len(resume_text.split())
        keyword_density = len(found_keywords) / max(word_count / 100, 1)
        
        # Combined score with density consideration
        final_score = (coverage * 0.8) + (min(keyword_density * 0.2, 0.2))
        return min(final_score, 1.0)
    
    def _calculate_ml_skill_score(self, skills: List[str], target_role: str) -> float:
        """ML Feature: Calculate skill relevance and quantity score"""
        if not skills:
            return 0.0
        
        # ML Component: Skill quantity scoring (more skills = better, to a point)
        skill_count = len(skills)
        if skill_count >= 12:
            quantity_score = 1.0
        elif skill_count >= 8:
            quantity_score = 0.8
        elif skill_count >= 5:
            quantity_score = 0.6
        elif skill_count >= 3:
            quantity_score = 0.4
        else:
            quantity_score = 0.2
        
        # ML Component: Skill relevance (basic implementation)
        # In production, this would use skill categorization and role matching
        relevance_score = 0.7  # Placeholder for skill-role relevance
        
        return (quantity_score * 0.6) + (relevance_score * 0.4)
    
    def _count_impact_verbs(self, resume_text: str) -> int:
        """ML Feature: Count impact-oriented verbs"""
        count = 0
        for verb in self.impact_verbs:
            count += len(re.findall(r'\b' + re.escape(verb) + r'\b', resume_text.lower()))
        return count
    
    def _generate_ml_feedback(self, structure_score: float, keyword_score: float, 
                            skill_score: float, sections: Dict[str, str], 
                            skills: List[str], target_role: str) -> List[str]:
        """ML Feature: Generate intelligent, actionable feedback"""
        feedback = []
        
        # Structure-based feedback
        if structure_score < 0.6:
            feedback.append("Improve resume structure: Ensure Experience, Education, and Skills sections are clearly defined.")
        
        missing_sections = []
        for section in ['experience', 'education', 'skills']:
            if section not in sections:
                missing_sections.append(section)
        
        if missing_sections:
            feedback.append(f"Add missing sections: {', '.join(missing_sections)}")
        
        # Keyword-based feedback
        if keyword_score < 0.6:
            feedback.append(f"Add more {target_role}-specific keywords to improve ATS compatibility and relevance.")
        elif keyword_score < 0.8:
            feedback.append("Good keyword coverage. Consider adding more technical terms and tools specific to your target role.")
        
        # Skills-based feedback
        if skill_score < 0.5:
            feedback.append("Include more technical skills relevant to your target role.")
        elif len(skills) > 15:
            feedback.append("Consider focusing on your most relevant skills (quality over quantity).")
        
        # Achievement-based feedback
        impact_count = self._count_impact_verbs(sections.get('experience', ''))
        if impact_count < 3:
            feedback.append("Use more action verbs and quantify achievements in your Experience section.")
        
        # Positive reinforcement
        if structure_score > 0.7:
            feedback.append("Well-structured resume with clear sections.")
        
        if keyword_score > 0.8:
            feedback.append("Excellent use of role-specific keywords.")
        
        return feedback[:6]  # Limit to most important feedback
    
    def explain(self, prediction: Dict[str, Any]) -> List[str]:
        """ML Feature: Explain the feedback analysis"""
        explanations = []
        overall_score = prediction.get('overall_score', 0)
        
        if overall_score >= 0.8:
            explanations.append("Excellent resume! Strong structure, keyword coverage, and skill presentation.")
        elif overall_score >= 0.6:
            explanations.append("Good resume foundation. Follow the specific suggestions to make it outstanding.")
        elif overall_score >= 0.4:
            explanations.append("Resume needs improvements in multiple areas. Focus on structure and role-specific content.")
        else:
            explanations.append("Significant improvements needed. Consider professional resume review and restructuring.")
        
        # Add metric-based insights
        metrics = prediction.get('metrics', {})
        impact_verbs = metrics.get('impact_verbs', 0)
        if impact_verbs < 2:
            explanations.append("Add more achievement-oriented language with quantifiable results.")
        
        return explanations
    
    def _get_empty_response(self) -> Dict[str, Any]:
        """ML Error Handling: Empty response"""
        return {
            "overall_score": 0.0,
            "structure_score": 0.0,
            "keyword_score": 0.0,
            "skill_score": 0.0,
            "metrics": {"word_count": 0, "character_count": 0, "skills_found": 0, "sections_found": 0, "impact_verbs": 0},
            "skills_found": [],
            "sections_found": [],
            "feedback": ["No resume text provided for analysis."],
            "model_version": self.get_version()
        }
    
    def _get_error_response(self, error_msg: str) -> Dict[str, Any]:
        """ML Error Handling: Error response"""
        return {
            "overall_score": 0.0,
            "structure_score": 0.0,
            "keyword_score": 0.0,
            "skill_score": 0.0,
            "metrics": {"word_count": 0, "character_count": 0, "skills_found": 0, "sections_found": 0, "impact_verbs": 0},
            "skills_found": [],
            "sections_found": [],
            "feedback": [f"Error analyzing resume: {error_msg}"],
            "model_version": self.get_version()
        }

# Global ML model instance
feedback_model = FeedbackModel()